# ğŸ¯ Servidor A2A para Trilhas de Estudos Personalizadas

Um servidor A2A (Agent-to-Agent Protocol) que utiliza LangChain e IA generativa para criar trilhas de estudos personalizadas baseadas no diagnÃ³stico de perfil profissional.

> âœ¨ **Novo**: Teste de integraÃ§Ã£o A2A com comunicaÃ§Ã£o entre agentes usando Ollama para decisÃµes inteligentes!

## ğŸ“‹ Funcionalidades

### ğŸ¤– Agent Skills (Habilidades do Agente)

1. **AnÃ¡lise de Perfil Profissional** - Analisa competÃªncias e identifica lacunas
2. **GeraÃ§Ã£o de Trilha de Estudos** - Cria trilhas personalizadas baseadas em objetivos
3. **SugestÃ£o de Recursos de Estudo** - Recomenda cursos, certificaÃ§Ãµes e materiais

### ğŸŒ Protocolo A2A

- **Agent Card** - Metadados e capacidades do agente
- **Task Management** - Gerenciamento de tarefas com estados
- **HTTP API** - Endpoints REST padronizados
- **SSE Support** - AtualizaÃ§Ãµes em tempo real via Server-Sent Events

### ğŸ§  IntegraÃ§Ãµes de IA

- **Google Gemini** (`langchain-google-genai`) - PadrÃ£o recomendado
- **Ollama** (`langchain-ollama>=0.3.3`) - Para execuÃ§Ã£o local
- **LangGraph** (`langgraph>=0.5.0`) - OrquestraÃ§Ã£o de agentes avanÃ§ados
- **Python A2A** (`python-a2a>=0.5.9`) - Protocolo de comunicaÃ§Ã£o entre agentes

### ğŸ“Š Interoperabilidade

CompatÃ­vel com o protocolo A2A para comunicaÃ§Ã£o entre agentes de IA.

## ğŸ§ª Teste de IntegraÃ§Ã£o A2A

### ğŸ¤– Demo de ComunicaÃ§Ã£o entre Agentes

IncluÃ­mos um teste completo que demonstra comunicaÃ§Ã£o A2A entre dois agentes:

- **Agente Principal**: Usa Ollama para decidir quando chamar outros agentes
- **Agente Auxiliar**: Fornece funcionalidades especÃ­ficas (cÃ¡lculo de scores de carreira)

### âš¡ ExecuÃ§Ã£o RÃ¡pida

```bash
# 1. Certifique-se que Ollama estÃ¡ rodando
ollama serve

# 2. Execute o teste de integraÃ§Ã£o
python run_test.py
```

### ğŸ¯ Como Funciona

```
ğŸ‘¤ UsuÃ¡rio: "Qual o score de carreira para tecnologia intermediÃ¡rio?"
    â†“
ğŸ¤– Agente Principal (porta 5000)
    â†“
ğŸ§  Ollama: "Precisa chamar agente auxiliar" âœ…
    â†“
ğŸ”§ Agente Auxiliar (porta 5001): Score 8/10 para tecnologia
    â†“
ğŸ“¤ Resposta combinada: DecisÃ£o IA + Resultado
```

### ğŸ“– DocumentaÃ§Ã£o Completa

Consulte [`TESTE_A2A.md`](./TESTE_A2A.md) para instruÃ§Ãµes detalhadas, exemplos e troubleshooting.

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1. PrÃ©-requisitos

- Python 3.12+
- `uv` (gerenciador de dependÃªncias da Astral)

### 2. InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio e navegue atÃ© o diretÃ³rio
cd career-path

# Instale as dependÃªncias
uv sync

# Ative o ambiente virtual
source .venv/bin/activate  # Linux/macOS
# ou
.venv\Scripts\activate     # Windows
```

### 3. ConfiguraÃ§Ã£o das VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
# ConfiguraÃ§Ãµes do Google Gemini (recomendado)
GOOGLE_API_KEY=sua_api_key_do_google

# ConfiguraÃ§Ãµes do Ollama (opcional)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1

# ConfiguraÃ§Ã£o padrÃ£o do provedor LLM
DEFAULT_LLM_PROVIDER=google  # ou "ollama"
```

#### Como obter a API Key do Google:

1. Acesse [Google AI Studio](https://makersuite.google.com/app/apikey)
2. FaÃ§a login com sua conta Google
3. Clique em "Create API Key"
4. Copie a chave gerada

## ğŸ“– Como Usar

### 1. Executando o Servidor Principal

```bash
# Servidor bÃ¡sico na porta 5000
python app/server.py

# Para desenvolvimento, tambÃ©m disponÃ­vel via mÃ³dulo:
python -m app.server
```

### 2. Skills DisponÃ­veis

O agente de carreira fornece as seguintes habilidades via protocolo A2A:

- **Analisar Perfil** - AnÃ¡lise detalhada de perfil profissional
- **Gerar Trilha de Estudos** - Trilhas personalizadas baseadas em objetivos
- **Sugerir Recursos** - RecomendaÃ§Ãµes de cursos, certificaÃ§Ãµes e materiais
- **Criar Perfil Exemplo** - Template para testes

### 3. Exemplo de Uso via HTTP

```bash
# AnÃ¡lise de perfil
curl -X POST http://localhost:5000/api/message \
  -H "Content-Type: application/json" \
  -d '{"text": "analisar perfil {...dados JSON...}"}'

# Criar exemplo de perfil
curl -X POST http://localhost:5000/api/message \
  -H "Content-Type: application/json" \
  -d '{"text": "exemplo"}'
```

### 2. Exemplo de Perfil de Entrada

```json
{
  "nome": "Ana Silva",
  "idade": 28,
  "escolaridade": "Superior Completo",
  "area_formacao": "AdministraÃ§Ã£o de Empresas",
  "competencias_atuais": [
    {
      "area": "Excel",
      "nivel": "AvanÃ§ado",
      "experiencia_anos": 5,
      "detalhes": "DomÃ­nio de fÃ³rmulas avanÃ§adas, tabelas dinÃ¢micas"
    },
    {
      "area": "Python",
      "nivel": "Iniciante",
      "experiencia_anos": 1,
      "detalhes": "Conhecimento bÃ¡sico em automaÃ§Ã£o"
    }
  ],
  "objetivos_carreira": [
    {
      "cargo_desejado": "Analista de Dados SÃªnior",
      "area_interesse": "CiÃªncia de Dados",
      "prazo_anos": 2,
      "motivacao": "Combinar negÃ³cios com anÃ¡lise de dados"
    }
  ],
  "disponibilidade_estudo_horas_semana": 10,
  "preferencia_aprendizado": "PrÃ¡tico com projetos reais",
  "recursos_disponiveis": "OrÃ§amento R$ 500/mÃªs para cursos"
}
```

### 4. Testando Localmente

```bash
# Execute o teste de integraÃ§Ã£o A2A completo
python run_test.py

# Teste especÃ­fico via cliente A2A
python test_integration_a2a.py teste
```

## ğŸ“ Estrutura do Projeto

```
career-path/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py            # Script principal CLI
â”‚   â”œâ”€â”€ server.py              # Servidor A2A principal
â”‚   â”œâ”€â”€ agent.py               # LÃ³gica do agente IA
â”‚   â””â”€â”€ models.py              # Modelos de dados Pydantic
â”œâ”€â”€ test_integration_a2a.py    # ğŸ§ª Teste de integraÃ§Ã£o A2A
â”œâ”€â”€ run_test.py                # ğŸš€ Script para executar testes
â”œâ”€â”€ TESTE_A2A.md              # ğŸ“– DocumentaÃ§Ã£o do teste A2A
â”œâ”€â”€ pyproject.toml             # ConfiguraÃ§Ã£o do projeto
â”œâ”€â”€ uv.lock                    # Lock file das dependÃªncias
â””â”€â”€ README.md                  # Esta documentaÃ§Ã£o
```

## ğŸ”§ Desenvolvimento

### Executar Testes

#### ğŸ§ª Teste de IntegraÃ§Ã£o A2A (Recomendado)

```bash
# Teste automÃ¡tico completo
python run_test.py

# Ou execute manualmente em 3 terminais:
# Terminal 1: python test_integration_a2a.py auxiliar
# Terminal 2: python test_integration_a2a.py principal
# Terminal 3: python test_integration_a2a.py teste
```

#### ğŸš€ Servidor Principal

```bash
# Inicie o servidor principal
python app/server.py

# Teste com curl
curl -X POST http://localhost:5000/api/message \
     -H "Content-Type: application/json" \
     -d '{"text": "Crie um exemplo de perfil"}'
```

### ConfiguraÃ§Ã£o com Ollama

Para usar Ollama localmente:

```bash
# Instalar ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Baixar modelo recomendado
ollama pull llama3.1

# Executar servidor
ollama serve
```

## ğŸ‰ ComeÃ§ar Agora

### ğŸ§ª Teste RÃ¡pido (Recomendado)

```bash
# 1. Instalar dependÃªncias
uv sync

# 2. Configurar Ollama
ollama serve

# 3. Executar teste de integraÃ§Ã£o A2A
python run_test.py
```

### ğŸš€ Para Desenvolvimento

```bash
# 1. Configurar variÃ¡veis de ambiente
cp .env.example .env
# Edite .env com suas API keys

# 2. Executar servidor principal
python app/server.py

# 3. Testar funcionalidades
python test_integration_a2a.py teste
```

### ğŸ“– PrÃ³ximos Passos

- **Para testar A2A**: Consulte [`TESTE_A2A.md`](./TESTE_A2A.md)
- **Para desenvolvimento**: Use o servidor principal em `app/server.py`
- **Para produÃ§Ã£o**: Configure autenticaÃ§Ã£o e deploy adequado

## ğŸ› ï¸ PersonalizaÃ§Ã£o

### Adicionar Novas Skills

1. Defina novos modelos em `app/models.py`
2. Implemente a lÃ³gica em `app/agent.py`
3. Adicione skills ao servidor em `app/server.py`
4. Teste com `test_integration_a2a.py`
5. Documente as novas funcionalidades
